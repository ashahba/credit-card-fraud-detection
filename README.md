
# Enhanced Fraud Detection using Graph Neural Networks
Boost fraud detection accuracy and developer efficiency through Intel's end-to-end, no-code, graph-neural-networks-boosted and multi-node distributed workflows.
Check out more workflow examples and reference implementations in the [Developer Catalog](https://developer.intel.com/aireferenceimplementations).
## **Table of Contents**
- [Overview](#overview)
- [Hardware requirements](#hardware-requirements)
- [How It Works](#how-it-works)
- [Getting Started](#getting-started)
- [Set Up and Run Single Node Pipeline with Docker](#set-up-and-run-single-node-pipeline-with-docker)
- [Developer Containers for Advanced Users](#developer-containers-for-advanced-users)
- [Run Docker Image in an Interactive Environment](#run-docker-image-in-an-interactive-environment)
- [Run Using Argo Workflows on K8s Using Helm](#run-using-argo-workflows-on-k8s-using-helm)
- [Set Up and Run Distributed Pipeline](#set-up-and-run-distributed-pipeline)
- [Expected Output](#expected-output)
- [Summary and Next Steps](#summary-and-next-steps)
- [Learn More](#learn-more)
- [Support](#support)
## Overview
Fraud detection has traditionally been tackled with classical machine learning algorithms such as gradient boosted machines. However, such supervised machine learning algorithms can lead to unsatisfactory precision and recall due to a few reasons:
- Severe class imbalance: ratio of fraud to non-fraud transactions is extremely imbalanced with typical values less than 1% 
- Complex fraudster behavior which evolves with time: it is quite difficult to capture user behavior using traditional ML techniques 
- Scale of data: credit card transaction datasets can have billions of transactions which require distributed preprocessing and training 
- Latency of fraud detection: it is important to detect fraud quickly in order to minimize losses, thus highlighting the need for distributed inference <br />
In Intel's Enhanced Fraud Detection reference kit, we employ Graph Neural Networks (GNN) popular for their ability to capture complex behavioral patterns (e.g., fraudsters performing multiple small transactions from different cards to not get caught). We also demonstrate a boost in accuracy by using GNN-boosted features over a baseline trained on traditional ML-only features. 
To make sure that we don't trade efficiency for accuracy, we enable distributed pipelines. Generally, distributed pipelines take weeks for data scientists to set up and involve numerous technical challenges. Our reference kit allows you to easily benefit from distributed capabilities and enjoy our no-code config-driven user interface. Additionally, we also provide ways for you to customize our solution to your own usecases.
### Highlights of Enhanced Fraud Detection Reference Use Case
- Significantly boost fraud classification accuracy by augmenting classical ML features with features generated through Graph Neural Networks (GNNs) 
- Utilize our distributed preprocessing, training and inference pipelines to detect fraud quickly
- Improve developer efficiency and experimentation with our no-code, config-driven user interface
To learn more, visit the [Credit Card Fraud Detection](https://github.com/intel/credit-card-fraud-detection) GitHub repository.
## Software Requirements 
Linux OS (Ubuntu 22.04) is used in this reference solution. Make sure the following dependencies are installed.
```bash
sudo apt update 
```
## Hardware Requirements
| Supported Hardware         | Precision  |
| ---------------------------- | ---------- |
| Intel® 1st, 2nd, 3rd, and 4th Gen Xeon® Scalable Performance processors | FP32 |
|Memory|>200GB|
|Storage|>50GB|
### Requirements for distributed pipelines
To benefit from our distributed pipeline, please ensure that -
1. Password-less ssh is set up on all the nodes.
2. A network file system (NFS) is set up for all the nodes to access.
3. A high-speed network between nodes is set up.
## How It Works
The high-level architecture of the reference use case is shown in the diagram below. We use a credit card transaction dataset open-sourced by IBM (commonly known as the tabformer dataset) in this reference use case to demonstrate the capabilities outlined in the Overview section. 
![folder-structure](assets/architecture.png)

### Task 1: Feature Engineering (Edge Featurization)
The feature engineering stage ingests the raw data, encodes each column into features using the logic defined in the feature engineering config yaml file and saves processed data. 
### Task 2: GNN Training (Node Featurization)
The GNN training stage creates homogenous graphs by consuming the processed data generated by Task 1 and trains a GraphSage model in a self-supervised link prediction task setting to learn the latent representations of the nodes (cards and merchants).  Once the GNN model is trained, the GNN workflow will concatenate the card and merchant features generated by the model to the corresponding transaction features and saves the GNN-boosted features to a CSV file.
### Task 3: XGBoost Training (Fraud Classification)
The XGBoost training stage trains a binary classification model using the data splitting, model parameters and runtime parameters set in the XGB training config yaml file. AUCPR (Area Under the Precision-Recall Curve) is used as the evaluation metric due to its robustness in evaluating highly imbalanced datasets. Data splitting is based on temporal sequence to simulate real-life scenario. The model performance on the tabformer dataset can be found in the table below.

## Getting Started
### 1. Set up the working directory 
Create a working directory for the use case. Use these commands to set up the log folder, data folder and corresponding subfolders inside the working directory. We assume that your working directory is work. 
```bash
export WORKDIR=$PWD/work
mkdir work && cd work
mkdir data && cd data
mkdir raw_data edge_data node_edge_data
```
If you are using the distributed pipeline, please repeat this step on localdisk of all nodes and on NFS. 
### 2. Download the Dataset 
How to get the tabformer dataset -
1. Download the transactions.tgz from https://github.com/IBM/TabFormer/tree/main/data/credit_card
2. Upload the transactions.tgz file to the `$WORKDIR/data/raw_data` folder in your working directory
3. Unzip the transactions.tgz file
```bash
cd $WORKDIR/data/raw_data
tar -zxvf transactions.tgz
```
If you want to bring your own dataset, put your raw data in the `$WORKDIR/data/raw_data` folder. If you are using the distributed pipeline, please repeat this step on localdisk of all nodes and on NFS. 
### 3. Download Fraud Detection Use Case Repository
Clone the repository into your working directory. 
```bash
cd $WORKDIR
git clone https://github.com/intel/credit-card-fraud-detection # to be replaced by external git link
cd credit-card-fraud-detection
git submodule update --init --recursive
```
If you are using the distributed pipeline, please repeat this step on localdisk of master node and on NFS. 
Your folder structure should follow the directory structure as shown in the figure below. 
![folder-structure](assets/folder_structure.jpg)
### Ways to run this reference use case
1. [Run with Docker on single node](#set-up-and-run-single-node-pipeline-with-docker)
2. [Run distributed pipelines](#set-up-and-run-distributed-pipeline) </br>
For advanced users, we provide options below:
1. [Developer containers for advanced users](#developer-containers-for-advanced-users)
2. [Run Docker Image in an Interactive Environment](#run-docker-image-in-an-interactive-environment)
3. [Run Using Argo Workflows on K8s Using Helm](#run-using-argo-workflows-on-k8s-using-helm)
---
## Set Up and Run Single Node Pipeline with Docker
To run distributed pipelines, please refer to [Run distributed pipelines](#set-up-and-run-distributed-pipeline). For advanced users, you can use [interactive mode](#run-docker-image-in-an-interactive-environment) to run the workflow containers. You can also check out the [Developer containers for advanced users](#dev-container) section to change the source code in the workflow containers to adapt to your own use case.
### 1. Set Up Docker Engine and Docker Compose
You'll need to install Docker Engine on your development system.
Note that while **Docker Engine** is free to use, **Docker Desktop** may require
you to purchase a license.  See the [Docker Engine Server installation
instructions](https://docs.docker.com/engine/install/#server) for details.
To build and run this workload inside a Docker Container, ensure you have Docker Compose installed on your machine. If you don't have this tool installed please consult official [Docker Compose installation documentation](https://docs.docker.com/compose/install/linux/#install-the-plugin-manually).
```bash
DOCKER_CONFIG=${DOCKER_CONFIG:-$HOME/.docker}
mkdir -p $DOCKER_CONFIG/cli-plugins
curl -SL https://github.com/docker/compose/releases/download/v2.7.0/docker-compose-linux-x86_64 -o $DOCKER_CONFIG/cli-plugins/docker-compose
chmod +x $DOCKER_CONFIG/cli-plugins/docker-compose
docker compose version
```
### 2. Setup Dataset
Ensure you have downloaded the dataset as described in [Prepare data directory](#2-download-the-dataset) and set the path to the dataset as described below.
```bash
export DATASET_DIR=<path-to-dataset>
```
### 3. Set Up Docker Image
You can choose to pull or build the containers that are going to be used to run the pipeline.
```bash
cd docker
docker compose build
```
OR
```bash
cd docker
docker pull intel/ai-workflows:eap-fraud-detection-classical-ml
docker pull intel/ai-workflows:eap-fraud-detection-gnn
```
### 4. Run single node pipeline
#### Step 1: Run Feature Engineering to get edge features
Run the preprocessing using the following command:
```bash
docker compose run preprocess 2>&1 | tee preprocess.log
```
The `preprocess` workflow will ingest the raw data in the `$DATASET_DIR/raw_data/card_transaction.v1.csv` directory, generate a preprocessed CSV file, and save it in the `$OUTPUT_DIR/data/edge_data/` directory. 
The table below shows some of the environment variables you can control according to your needs.
| Environment Variable Name | Default Value | Description |
| --- | --- | --- |
| CONFIG_DIR | `$PWD/../configs/single-node` | Directory of the config file |
| OUTPUT_DIR | `$PWD/output` | Output Preprocessed Dataset directory. |
#### Step 2: Train and evaluate XGBoost baseline model with edge features only for fraud classification
The preprocess pipeline must complete successfully before running the baseline training. The preprocess pipeline generates an output CSV file at `$OUTPUT_DIR/data/edge_data/`.
The `baseline-training` workflow will consume the CSV file generated from `preprocess` above, and run a training of a XGBoost model. It will also print out AUCPR (area under the precision-recall curve) results to the console. 
![baseline_topology](https://user-images.githubusercontent.com/18349036/226064639-c56a8d8b-c514-4a3e-ab72-a9903df7d9ff.png)
Run the workflow container with the command below.
```bash
docker compose run baseline-training 2>&1 | tee baseline-training.log
```
The table below shows some of the environment variables you can control according to your needs.
| Environment Variable Name | Default Value | Description |
| --- | --- | --- |
| OUTPUT_DIR | `$PWD/output` | Output directory. Should be the same as set in [Step-2](#step-2-train-and-evaluate-xgboost-baseline-model-with-edge-features-only-for-fraud-classification)  |
#### Step 3: Train and Evaluate XGBoost with both edge features and GNN generated node features for Fraud Classification
To see the improvement over the baseline training you can run the xgb-training pipeline. Before running the baseline training, the preprocess pipeline must complete successfully. The preprocess pipeline generates an output CSV file at `$OUTPUT_DIR/data/edge_data/`. The `xgb-training` workflow consumes the CSV file generated from `preprocess` above, runs the `gnn-analytics` pipeline to generate optimized features, and runs a training of a XGBoost model using these features. It will also print out AUCPR (area under the precision-recall curve) results to the console.
![stock_xgb_topology](https://user-images.githubusercontent.com/18349036/226065312-76bc9fb6-8a8d-4008-9245-26a1869492b6.png)
Run the workflow container with the command below.
```bash
docker compose run xgb-training 2>&1 | tee xgb-training.log
```
This command runs the `gnn-analytics` pipeline to generate the node features first and then uses edge features generated from [Step 2](#step-2-train-and-evaluate-xgboost-baseline-model-with-edge-features-only-for-fraud-classification) to train the XGBoost model and will print out AUCPR (area under the precision-recall curve) results to the console.
The table below shows some of the environment variables you can control according to your needs.
| Environment Variable Name | Default Value | Description |
| --- | --- | --- |
| CONFIG_DIR | `$PWD/../configs/single-node` | Directory of the config file |
| OUTPUT_DIR | `$PWD/output` | Dataset, Logfile and Checkpoint output. Should contain `${OUTPUT_DIR}/data/edge_data/processed_data.csv`  |
### 5. Logs
You can also check logs of different pipelines using the following commands.
```bash
docker compose logs gnn-analytics -f
```
Run these commands to check the preprocess, baseline, and xgb-training logs:
```bash
cat preprocess.log
cat baseline-training.log
cat xgb-training.log
```
## Developer containers for advanced users
If you want to edit and run your own version of a workflow with the same docker containers you can use the dev pipelines below. The environment variables you can control are described below. 
### Developer Baseline Container
This pipeline allows you to control the source code, scripts, and parameters for running the baseline XGBoost pipeline. This takes a preprocess dataset and trains an XGBoost model.
```bash
docker compose run dev-baseline-training
```
| Environment Variable Name | Default Value | Description |
| --- | --- | --- |
| CONFIG_DIR | `$PWD/../configs/single-node` | Directory of the config file |
| OUTPUT_DIR | `$PWD/output` | Dataset, Logfile and Checkpoint output. Should contain `${OUTPUT_DIR}/data/edge_data/processed_data.csv`  |
| PARAMETER | `--baseline-training` | Script file parameter |
| SCRIPT | `/fraud-detection/wrapper.sh` | Path of Script inside the container.|
| WORKSPACE | `$PWD/../classical-ml` | Submodule Location of the classical-ml |
### Developer GNN Container
This pipeline allows you to control the source code, scripts, and parameters for running the GNN Analytics pipeline. This takes a preprocess dataset and generates the node features described in [GNN Training](#task-2-gnn-training-node-featurization).
```bash
docker compose run dev-gnn-analytics
```
| Environment Variable Name | Default Value | Description |
| --- | --- | --- |
| OUTPUT_DIR | `$PWD/output` | Dataset, Logfile and Checkpoint output. Should contain `${OUTPUT_DIR}/data/edge_data/processed_data.csv`  |
| PARAMETER | `--gnn-analytics` | Script file parameter |
| SCRIPT | `/fraud-detection/wrapper.sh` | Path of Script inside the container.|
| WORKSPACE | `$PWD/../gnn-analytics` | Submodule Location of the gnn-workflow|
### Developer XGBoost Training Container
This pipeline allows you to control the source code, scripts, and parameters for running the GNN Boosted XGBoost training pipeline. This takes the GNN boosted dataset and runs the training described in [XGBoost Training](#task-3-xgboost-training-fraud-classification).
```bash
docker compose run dev-xgb-training
```
| Environment Variable Name | Default Value | Description |
| --- | --- | --- |
| CONFIG_DIR | `$PWD/../configs/single-node` | Directory of the config file |
| OUTPUT_DIR | `$PWD/output` | Dataset, Logfile and Checkpoint output. Should contain `${OUTPUT_DIR}/data/edge_data/processed_data.csv`  |
| PARAMETER | `--xgb-training` | Script file parameter |
| SCRIPT | `/fraud-detection/wrapper.sh` | Path of Script inside the container.|
| WORKSPACE | `$PWD/../classical-ml` | Submodule Location of the classical-ml |
## Clean Up Docker Containers
Run the following command to stop all services and containers created by docker compose and remove them.
```bash
docker compose down
```
## Run Docker Image in an Interactive Environment
You can also choose to run the container in an interactive manner to execute the workflows manually after getting inside the containers.
If your environment requires a proxy to access the internet, export your development system's proxy settings to the docker environment:
```bash
export DOCKER_RUN_ENVS="-e ftp_proxy=${ftp_proxy} \
  -e FTP_PROXY=${FTP_PROXY} -e http_proxy=${http_proxy} \
  -e HTTP_PROXY=${HTTP_PROXY} -e https_proxy=${https_proxy} \
  -e HTTPS_PROXY=${HTTPS_PROXY} -e no_proxy=${no_proxy} \
  -e NO_PROXY=${NO_PROXY} -e socks_proxy=${socks_proxy} \
  -e SOCKS_PROXY=${SOCKS_PROXY}"
```
### Run the baseline-training workflow
Use the following command to run the `baseline-training` workflow container interactively.
```bash
export DATASET_DIR=/path/to/data
export OUTPUT_DIR=$PWD/output
export WORKSPACE=$PWD/../classical-ml
export WRAPPER_SCRIPT=$PWD/../wrapper.sh
export CONFIG_DIR=$PWD/../configs/single-node
docker run -a stdout ${DOCKER_RUN_ENVS} \
           -v ${DATASET_DIR}/raw_data:/fraud-detection/data/raw_data \
           -v ${CONFIG_DIR}:/fraud-detection/configs \
           -v ${OUTPUT_DIR}/data/edge_data:/fraud-detection/data/edge_data \
           -v ${OUTPUT_DIR}/baseline/models:/workspace/tmp/models \
           -v ${WORKSPACE}:/fraud-detection/classical-ml \
           --privileged --init -it --rm --pull always \
           -w /fraud-detection/classical-ml \
           intel/ai-workflows:eap-fraud-detection-classical-ml \
           bash
```
Run the command below for preprocessing and baseline training
```bash
/fraud-detection/wrapper.sh --preprocess --baseline-training
```
## Run Using Argo Workflows on K8s Using Helm
### 1. Install Helm
- Install [Helm](https://helm.sh/docs/intro/install/)
```bash
curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 && \
chmod 700 get_helm.sh && \
./get_helm.sh
```
### 2. Setting up K8s
- Install [Argo Workflows](https://argoproj.github.io/argo-workflows/quick-start/) and [Argo CLI](https://github.com/argoproj/argo-workflows/releases)
- Configure your [Artifact Repository](https://argoproj.github.io/argo-workflows/configure-artifact-repository/)
### 3. Install Workflow Template
Ensure that you have reviewed the [Hardware Requirements](#hardware-requirements) and each of your nodes has sufficient memory for the GNN workflow.
```bash
export NAMESPACE=argo
helm install --namespace ${NAMESPACE} --set proxy=${http_proxy} fraud-detection ./chart
argo submit --from wftmpl/fraud-detection --namespace=${NAMESPACE}
```
### 4. View 
To view your workflow progress
```bash
argo logs @latest -f
```
## Set up and run distributed pipeline 
To run with Docker on single node, refer to [Run with Docker on single node](#set-up-and-run-single-node-pipeline-with-docker).
### 1. Requirements
1. Password-less ssh needs to be set up on all the nodes that you are using. 
2. GNN workflow requires the data directory and code repository to reside on Network File System (NFS). 
3. Classical ML workflow requires code, configs and data directory to reside locally. 
4. The nodes should be connected with a high-speed network to enjoy speedups.
### 2. Set up distributed workflows
#### Step 1: set up docker compose (on master node)
Please set up docker compose using the instructions mentioned in section [Set up docker engine and docker compose](#1-set-up-docker-engine-and-docker-compose). 

#### Step 2: Set up distributed Classical ML workflow 
Build the docker image on master node, and copy the same docker image `spark-image.tar` over to all the worker nodes. 
```bash
# on master node
cd $WORKDIR/fraud-detection-usecase/docker
docker compose build
docker save -o <path/to/save>/spark-image.tar classical-ml-wf
scp <path/to/save>/spark-image.tar <user_id>@<worker node ip>:<worker/node/path/to/save>/spark-image.tar
```
Then unpack the docker image on the worker node.  
```bash
# on worker node 
docker load -i <worker/node/path/to/save>/spark-image.tar
```
#### Step 3: Setup distributed GNN workflow
Please follow the instructions on [GNN workflow](https://github.com/intel/graph-neural-networks-and-analytics/eap-release#run-bare-metal-on-a-cluster-of-machines) GitHub repository to set up the distributed GNN workflow.
### 3. Run the Distributed Workflows
#### Step 1: Run distributed preprocessing
1. Prepare the workflow config yaml (workflow-data-preprocessing.yaml) such that it reflects the number of nodes you wish to use and their IP addresses. </br>
```bash
env:
  num_node: 2
  node_ips: #the first item in the ip list is the master ip, pls make sure that the ip doesn't contain space in the end
    - IP1
    - IP2
  # If you followed our setup instructions, you can get <path-to-work-dir> by running $WORKDIR in your terminal
  tmp_path: <path-to-work-dir> 
  data_path: <path-to-work-dir>/data
  config_path: <path-to-work-dir>/fraud-detection-usecase/configs/distributed
``` 
2. Pass the workflow config yaml to the Classical ML workflow container and launch the workflow container from master node with the command below. 
```bash
# on master node in localdisk
cd $WORKDIR/fraud-detection-usecase/classical-ml
./start-workflow.sh $WORKDIR/fraud-detection-usecase/configs/distributed/workflow-data-preprocessing.yaml
```
The Classical ML workflow saves the processed data on the local disk of the master node.
#### Step 2: Train distributed baseline model (edge features only)
1. Prepare the workflow config yaml (workflow-baseline.yaml) that specifies the number of nodes and their IP addresses.  </br>
```bash
env:
  num_node: 2
  node_ips: #the first item in the ip list is the master ip, pls make sure that the ip doesn't contain space in the end
    - IP1
    - IP2
  # If you followed our setup instructions, you can get <path-to-work-dir> by running $WORKDIR in your terminal
  tmp_path: <path-to-work-dir> 
  data_path: <path-to-work-dir>/data
  config_path: <path-to-work-dir>/fraud-detection-usecase/configs/distributed
```
2. Pass the workflow config yaml to the Classical ML workflow container and run the workflow container with the command below.
```bash
# on master node in localdisk
cd $WORKDIR/fraud-detection-usecase/classical-ml
./start-workflow.sh $WORKDIR/fraud-detection-usecase/configs/distributed/workflow-baseline.yaml
```
#### Step 3. Train distributed Graph Neural Network to get node features 
Please refer to the [GNN workflow Run Bare Metal instructions](https://github.com/intel/graph-neural-networks-and-analytics/tree/v1.0-eap#run-bare-metal-on-a-cluster-of-machines). </br>
Before running the distributed GNN workflow, copy the processed_data folder to the NFS.</br>
After the distributed GNN workflow is completed, copy the the data file generated by the GNN workflow from NFS to the gnn_boosted_data folder on the master node for the final XGBoost model training with the Classical ML workflow.
#### Step 4. Distributed XGBoost training for fraud classification
1. Prepare the workflow config yaml (workflow-xgb-training.yaml) that specifies the parameters for running the workflow for final XGBoost model training. </br>
```bash
env:
  num_node: 2
  node_ips: #the first item in the ip list is the master ip, pls make sure that the ip doesn't contain space in the end
    - IP1
    - IP2
  # If you followed our setup instructions, you can get <path-to-work-dir> by running $WORKDIR in your terminal
  tmp_path: <path-to-work-dir>  
  data_path: <path-to-work-dir>/data
  config_path: <path-to-work-dir>/fraud-detection-usecase/configs/distributed
```
2. Pass the workflow config yaml to the Classical ML workflow container and run the workflow container with the command below.
```bash
# on master node in localdisk
cd $WORKDIR/fraud-detection-usecase/classical-ml
./start-workflow.sh $WORKDIR/fraud-detection-usecase/configs/distributed/workflow-xgb-training.yaml
```
---
## Expected output 
### 1: Results 
We use Area Under the Precision Recall Curve (AUCPR) as evaluation metric (lies between 0 and 1, higher is better). 
#### Results for single node pipeline
You should expect to see a boost in AUCPR for test split by using GNN-boosted features. 
| Data split                     | Number of examples   | AUCPR - Edge features only   | AUCPR - GNN-boosted features   |
| :----------------------------: | :-----------------------: | :-------------------------: | :----------------------------: |
|   Train (year < 2018)          |     20,604,847            |            0.95             |           0.99                 |
|    Val (year = 2018)           |      1,689,822            |            0.88             |           0.88                 |
|   Test (year > 2018)           |      1,904,167            |            0.79             |           0.83                 |
#### Results for distributed pipeline
You should expect to see a boost in AUCPR for test split by using GNN-boosted features. 
| Data split                     | Number of examples   | AUCPR - Edge features only   | AUCPR - GNN-boosted features   |
| :----------------------------: | :-----------------------: | :-------------------------: | :----------------------------: |
|   Train (year < 2018)          |     20,604,847            |            0.95             |           0.99                 |
|    Val (year = 2018)           |      1,689,822            |            0.87             |           0.87                 |
|   Test (year > 2018)           |      1,904,167            |            0.82             |           0.86                 |
### 2: Expected output for single node pipeline 
You will see logs that look similar to the ones below once you run the use case successfully. Please note that the timing numbers depend on the hardware systems.
#### Expected output for single node preprocessing
```
Preprocessing dataset
Failed to read model training configurations. This is either due to wrong parameters defined in the config file as shown: 'training'
Or there is no need for model training.
enter single-node mode...
prepare env took 0.0 seconds
reading data...
(24386900, 15)
dp read data took 43.0 seconds
preparing data...
dp prepare data took 5.0 seconds
engineering features...
dp engineer features took 120.4 seconds
splitting data...
dp split data took 4.8 seconds
encoding features...
dp encode features took 96.1 seconds
saving data...
data saved under the path /fraud-detection/data/edge_data/processed_data.csv
dp save data took 259.1 seconds
data preprocessing took 528.4 seconds
The whole workflow processing took 528.4 seconds
```
#### Expected output for single node baseline
```
regenerate workflow tmp folders....
Failed to read data preprocessing steps. This is either due to wrong parameters defined in the config file as shown: 'data_preprocess'
Or there is no need for data preprocessing.
enter single-node mode...
prepare env took 0.0 seconds
start training models soon...
read and prepare data for training...
(24198836, 22)
start xgboost model training...
[0]     train-aucpr:0.30292     valid-aucpr:0.18852     test-aucpr:0.18230
[100]   train-aucpr:0.75583     valid-aucpr:0.80220     test-aucpr:0.68338
[200]   train-aucpr:0.84341     valid-aucpr:0.82412     test-aucpr:0.71389
[300]   train-aucpr:0.86998     valid-aucpr:0.86242     test-aucpr:0.75954
[400]   train-aucpr:0.88903     valid-aucpr:0.86131     test-aucpr:0.76025
[500]   train-aucpr:0.90314     valid-aucpr:0.88119     test-aucpr:0.78382
[600]   train-aucpr:0.91482     valid-aucpr:0.88324     test-aucpr:0.78819
[700]   train-aucpr:0.92490     valid-aucpr:0.88226     test-aucpr:0.79116
[800]   train-aucpr:0.93327     valid-aucpr:0.88355     test-aucpr:0.79309
[900]   train-aucpr:0.94046     valid-aucpr:0.88276     test-aucpr:0.79143
[999]   train-aucpr:0.94649     valid-aucpr:0.88188     test-aucpr:0.79205
xgboost is saved.
training took 4229.2 seconds
The whole workflow processing took 4229.2 seconds
```
#### Expected output for single node GNN and XGB training
```
Failed to read data preprocessing steps. This is either due to wrong parameters defined in the config file as shown: 'data_preprocess'
Or there is no need for data preprocessing.
enter single-node mode...
prepare env took 0.0 seconds
start training models soon...
read and prepare data for training...
(24198836, 150)
start xgboost model training...
[0] train-aucpr:0.31415 valid-aucpr:0.11117 test-aucpr:0.09945
[100] train-aucpr:0.81719 valid-aucpr:0.85195 test-aucpr:0.76542
[200] train-aucpr:0.88523 valid-aucpr:0.86021 test-aucpr:0.79006
[300] train-aucpr:0.92122 valid-aucpr:0.88141 test-aucpr:0.81731
[400] train-aucpr:0.94534 valid-aucpr:0.89083 test-aucpr:0.82701
[500] train-aucpr:0.96092 valid-aucpr:0.88692 test-aucpr:0.82644
[600] train-aucpr:0.97157 valid-aucpr:0.88517 test-aucpr:0.82724
[700] train-aucpr:0.97936 valid-aucpr:0.88205 test-aucpr:0.82191
[800] train-aucpr:0.98486 valid-aucpr:0.88132 test-aucpr:0.82018
[900] train-aucpr:0.98910 valid-aucpr:0.87862 test-aucpr:0.81839
[999] train-aucpr:0.99258 valid-aucpr:0.88680 test-aucpr:0.82811
xgboost is saved.
training took 8308.5 seconds
The whole workflow processing took 8308.5 seconds
```

---
## Summary and next steps
The steps above demonstrate accuracy boost through using GNN's and efficiency boost through setting up distributed pipelines as well as a no-code user experience through configs. To experiment further with Intel's enhanced fraud detection reference use case, you can - 
1. Bring your own dataset: You can add your own source data to `/data/raw_data`. 
2. Bring your own preprocessor : You can create your own preprocessor (i.e. edge featurizer) by editing `data-preprocessing.yaml`. More information on how to write config yaml file can be found in the [Classical ML workflow](https://github.com/intel/recommender-system-with-distributed-classical-ml) GitHub repository.
3. Bring your own baseline : You can set parameters of your baseline model such as learning_rate, eval_metric, num_boost_round, verbose_eval and so on in `baseline-xgb-training.yaml`. 
4. Bring your own XGB model : You can set parameters of your final XGB model such as learning_rate, eval_metric, num_boost_round, verbose_eval and so on in `xgb-training.yaml` if you want to edit final XGB model. 
Make sure to edit your configs in `/configs/single-node` folder if you're using single-node setting and `/configs/distributed` if you're using distributed setting. 
---
## Learn more
To learn more about our workflows, please refer to -
1. [Classical ML workflow](https://github.com/intel/recommender-system-with-distributed-classical-ml)
2. [GNN workflow](https://github.com/intel/graph-neural-networks-and-analytics)
## Support
To troubleshoot, please submit your github issue. 
